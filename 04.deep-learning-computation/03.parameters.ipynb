{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "940f56fc-0303-4e0e-b63c-40603fb0be32",
   "metadata": {},
   "source": [
    "# 参数管理\n",
    "\n",
    "在选择了架构并设置了超参数后，我们就进入了训练阶段。\n",
    "此时，我们的目标是找到使损失函数最小化的模型参数值。\n",
    "经过训练后，我们将需要使用这些参数来做出未来的预测。\n",
    "此外，有时我们希望提取参数，以便在其他环境中复用它们，\n",
    "将模型保存下来，以便它可以在其他软件中执行，\n",
    "或者为了获得科学的理解而进行检查。\n",
    "\n",
    "之前的介绍中，我们只依靠深度学习框架来完成训练的工作，\n",
    "而忽略了操作参数的具体细节。\n",
    "本节，我们将介绍以下内容：\n",
    "\n",
    "* 访问参数，用于调试、诊断和可视化；\n",
    "* 参数初始化；\n",
    "* 在不同模型组件间共享参数。\n",
    "\n",
    "(**我们首先看一下具有单隐藏层的多层感知机。**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37f1fc29-0980-42c2-89f9-838902edeba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9473, 0.1408, 0.5314, 0.4237],\n",
       "         [0.1545, 0.3516, 0.3701, 0.4959]]),\n",
       " tensor([[0.4900],\n",
       "         [0.2609]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "\n",
    "X, net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69c885a-d0e6-4220-9e75-7b31fcd67770",
   "metadata": {},
   "source": [
    "## [**参数访问**]\n",
    "\n",
    "我们从已有模型中访问参数。\n",
    "当通过`Sequential`类定义模型时，\n",
    "我们可以通过索引来访问模型的任意层。\n",
    "这就像模型是一个列表一样，每层的参数都在其属性中。\n",
    "如下所示，我们可以检查第二个全连接层的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aa27910-7e0d-4ad1-9a98-5b1ceb3be7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-0.1499,  0.3554,  0.3243, -0.1847],\n",
      "        [ 0.0455,  0.3521, -0.2007,  0.0680],\n",
      "        [ 0.4860, -0.2394,  0.4372, -0.0590],\n",
      "        [ 0.3551,  0.2468, -0.2327,  0.1194],\n",
      "        [-0.3683, -0.0791,  0.4001,  0.2472],\n",
      "        [-0.4399,  0.2609, -0.0626, -0.1922],\n",
      "        [ 0.4200, -0.4823, -0.1344,  0.1079],\n",
      "        [ 0.0541,  0.1088, -0.4551, -0.4700]])), ('bias', tensor([ 0.0601, -0.2213,  0.3355,  0.2891,  0.3082,  0.2305,  0.1661,  0.3045]))])\n"
     ]
    }
   ],
   "source": [
    "print(net[0].state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d874552-7e2e-40d7-a834-d6f78cb3540b",
   "metadata": {},
   "source": [
    "输出的结果告诉我们一些重要的事情：\n",
    "首先，这个全连接层包含两个参数，分别是该层的权重和偏置。\n",
    "两者都存储为单精度浮点数（float32）。\n",
    "注意，参数名称允许唯一标识每个参数，即使在包含数百个层的网络中也是如此。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a3a212-476d-40b6-966f-7f9a2fe3e77a",
   "metadata": {},
   "source": [
    "### [**目标函数**]\n",
    "\n",
    "注意，每个参数都表示为参数类的一个实例。\n",
    "要对参数执行任何操作，首先我们需要访问底层的数值。\n",
    "有几种方法可以做到这一点。有些比较简单，而另一些则比较通用。\n",
    "下面的代码从第二个全连接层（即第三个神经网络层）提取偏置，\n",
    "提取后返回的是一个参数类实例，并进一步访问该参数的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2df6b7ff-4ea2-43c5-b915-419a22cecfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.1968], requires_grad=True)\n",
      "tensor([0.1968])\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d110a59-7f00-4e82-8980-ad80ace5405f",
   "metadata": {},
   "source": [
    "参数是复合的对象，包含值、梯度和额外信息。\n",
    "这就是我们需要显式参数值的原因。\n",
    "除了值之外，我们还可以访问每个参数的梯度。\n",
    "在上面这个网络中，由于我们还没有调用反向传播，所以参数的梯度处于初始状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cfd9421-2178-4934-b821-244f6c943946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.grad == None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8b1bf9-75b2-44d7-b16c-eb0ae664a582",
   "metadata": {},
   "source": [
    "### [**一次性访问所有参数**]\n",
    "\n",
    "当我们需要对所有参数执行操作时，逐个访问它们可能会很麻烦。\n",
    "当我们处理更复杂的块（例如，嵌套块）时，情况可能会变得特别复杂，\n",
    "因为我们需要递归整个树来提取每个子块的参数。\n",
    "下面，我们将通过演示来比较访问第一个全连接层的参数和访问所有层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd02820e-a64e-4479-a410-7c39a1bad945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0244e7a-20f6-49dd-9573-cf7d36340278",
   "metadata": {},
   "source": [
    "[为什么0.weight的size是[8, 4], 2.weight的size是[1, 8]，而且没有1.weight](https://kimi.moonshot.cn/share/cql4u0mc2kuhg8966hg0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9065d987-d885-495b-b73b-52a65769d01d",
   "metadata": {},
   "source": [
    "这为我们提供了另一种访问网络参数的方式，如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05674d4a-a923-479c-8c95-0568b8a56483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1968])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['2.bias'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9e4a5e-c395-458a-b6e6-4643a0c52523",
   "metadata": {},
   "source": [
    "### [**从嵌套块收集参数**]\n",
    "\n",
    "让我们看看，如果我们将多个块相互嵌套，参数命名约定是如何工作的。\n",
    "我们首先定义一个生成块的函数（可以说是“块工厂”），然后将这些块组合到更大的块中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8316a969-cb73-4103-aeb8-6564e0c54d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4482],\n",
       "        [0.4482]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(), \n",
    "                        nn.Linear(8, 4), nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # 在这里嵌套\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950d2194-0f0d-4e6b-a4a3-0528fd3453b9",
   "metadata": {},
   "source": [
    "[**设计了网络后，我们看看它是如何工作的。**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "479ad9e0-2642-4289-bae1-7864bf2bbf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d106752a-1b4d-4973-bf1e-18f268232766",
   "metadata": {},
   "source": [
    "因为层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们。\n",
    "下面，我们访问第一个主要的块中、第二个子块的第一层的偏置项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b97fd0fd-09cd-43bf-b14a-9710ebc6b4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1229, -0.1921,  0.2069, -0.0311, -0.4421,  0.4749, -0.3397,  0.2542])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet[0][1][0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fde2c4-ddac-4286-876d-9f9f1af9b730",
   "metadata": {},
   "source": [
    "## 参数初始化\n",
    "\n",
    "知道了如何访问参数后，现在我们看看如何正确地初始化参数。\n",
    "我们在[数值稳定性](https://github.com/lixinjie97/Deep_learning_tutorial/blob/main/03.multilayer-perceptrons/09.numerical-stability-and-init.ipynb)中讨论了良好初始化的必要性。\n",
    "深度学习框架提供默认随机初始化，\n",
    "也允许我们创建自定义初始化方法，\n",
    "满足我们通过其他规则实现初始化权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc9baba-cf38-477b-ba6f-1d908b5de2ce",
   "metadata": {},
   "source": [
    "默认情况下，PyTorch会根据一个范围均匀地初始化权重和偏置矩阵，\n",
    "这个范围是根据输入和输出维度计算出的。\n",
    "PyTorch的`nn.init`模块提供了多种预置初始化方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6babc36d-58d1-414e-a55a-369e4174a994",
   "metadata": {},
   "source": [
    "### [**内置初始化**]\n",
    "\n",
    "让我们首先调用内置的初始化器。\n",
    "下面的代码将所有权重参数初始化为标准差为0.01的高斯随机变量，\n",
    "且将偏置参数设置为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da41e624-f690-446b-945b-0dfcbb96e66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0047, -0.0136, -0.0024, -0.0033]), tensor(0.))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6741aed-c39c-4203-ab13-c7bfe916988f",
   "metadata": {},
   "source": [
    "我们还可以将所有参数初始化为给定的常数，比如初始化为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deadce82-cada-4d70-a2d8-dfbdbc190df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4915fc22-c958-411c-82c6-ca67cb18964b",
   "metadata": {},
   "source": [
    "我们还可以[**对某些块应用不同的初始化方法**]。\n",
    "例如，下面我们使用Xavier初始化方法初始化第一个神经网络层，\n",
    "然后将第三个神经网络层初始化为常量值42。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5da55c9c-4d69-479b-a3c2-2d35d7c49d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4538, -0.3270, -0.1785, -0.3643])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "def init_xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "\n",
    "net[0].apply(init_xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a7d28a-7f21-45d0-9c5c-f0b77aaaf677",
   "metadata": {},
   "source": [
    "### [**自定义初始化**]\n",
    "\n",
    "有时，深度学习框架没有提供我们需要的初始化方法。\n",
    "在下面的例子中，我们使用以下的分布为任意权重参数$w$定义初始化方法：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    w \\sim \\begin{cases}\n",
    "        U(5, 10) & \\text{ 可能性 } \\frac{1}{4} \\\\\n",
    "            0    & \\text{ 可能性 } \\frac{1}{2} \\\\\n",
    "        U(-10, -5) & \\text{ 可能性 } \\frac{1}{4}\n",
    "    \\end{cases}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1467e509-cabe-4abb-a6ac-f4c88fba0e65",
   "metadata": {},
   "source": [
    "同样，我们实现了一个`my_init`函数来应用到`net`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ee7bf45-a76f-48ee-bae5-97d6c10f843e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000, 0.0000, 7.9345, -0.0000],\n",
       "        [8.6410, 0.0000, 0.0000, -0.0000]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape)\n",
    "                       for name, param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a846f94c-23ac-4d31-8889-9173a97b9fa8",
   "metadata": {},
   "source": [
    "注意，我们始终可以直接设置参数。\n",
    "\n",
    "[解释一下上面的初始化权重](https://kimi.moonshot.cn/share/cql78augi3pjqbnn94kg)\n",
    "\n",
    "[补充](https://kimi.moonshot.cn/share/cql798u65ra1tij7fc00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ce90a41-71fe-432b-b035-6b3fa6d7f5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.0000,  1.0000,  8.9345,  1.0000])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 42\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9287a3-45d6-4b3c-90e1-f0618d98c65b",
   "metadata": {},
   "source": [
    "## [**参数绑定**]\n",
    "\n",
    "有时我们希望在多个层间共享参数：\n",
    "我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d6655d2-6137-4e4b-97b5-0d57993f0d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# 我们需要给共享层一个名称，以便可以引用它的参数\n",
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                   shared, nn.ReLU(),\n",
    "                   shared, nn.ReLU(),\n",
    "                   nn.Linear(8, 1))\n",
    "net(X)\n",
    "# 检查参数是否相同\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "# 确保它们实际上是同一个对象而不只是有相同的值\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379ce9a1-0a67-4de5-9dd7-f8c2e27b9daa",
   "metadata": {},
   "source": [
    "这个例子表明第三个和第五个神经网络层的参数是绑定的。\n",
    "它们不仅值相等，而且由相同的张量表示。\n",
    "因此，如果我们改变其中一个参数，另一个参数也会改变。\n",
    "这里有一个问题：当参数绑定时，梯度会发生什么情况？\n",
    "答案是由于模型参数包含梯度，因此在反向传播期间第二个隐藏层（即第三个神经网络层）和第三个隐藏层（即第五个神经网络层）的梯度会加在一起。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401630e1-1109-4acc-b51d-5cd77dd98ca1",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 我们有几种方法可以访问、初始化和绑定模型参数。\n",
    "* 我们可以使用自定义初始化方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96dc9b1-e39e-45c7-a18a-9249b2f6abf1",
   "metadata": {},
   "source": [
    "## 练习\n",
    "\n",
    "1. 使用[层和块](https://github.com/lixinjie97/Deep_learning_tutorial/blob/main/04.deep-learning-computation/02.model-construction.ipynb)中定义的`NestMLP`模型，访问各个层的参数。\n",
    "1. 查看初始化模块文档以了解不同的初始化方法。\n",
    "1. 构建包含共享参数层的多层感知机并对其进行训练。在训练过程中，观察模型各层的参数和梯度。\n",
    "1. 为什么共享参数是个好主意？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f26007a-6485-49f9-82a3-bd95a115a0ae",
   "metadata": {},
   "source": [
    "### 练习一\n",
    "\n",
    "1. 使用[层和块](https://github.com/lixinjie97/Deep_learning_tutorial/blob/main/04.deep-learning-computation/02.model-construction.ipynb)中定义的`NestMLP`模型，访问各个层的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6152faa-99d2-4dd4-8b0b-4eba46f17c4d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;引用上节中的`NestMLP`模型，可以使用以下代码访问该模型各个层的参数，输出结果将显示每个层对应的参数名称、形状和具体参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "938819cd-b5f0-4767-ab3f-b35d72bed8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "访问net层的参数\n",
      "参数名称：0.weight, 形状：torch.Size([64, 20])\n",
      "参数名称：0.bias, 形状：torch.Size([64])\n",
      "参数名称：2.weight, 形状：torch.Size([20, 64])\n",
      "参数名称：2.bias, 形状：torch.Size([20])\n",
      "\n",
      "访问linear层的参数\n",
      "参数名称：weight, 形状：torch.Size([16, 32])\n",
      "参数名称：bias, 形状：torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                nn.Linear(64, 20), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "model = NestMLP()\n",
    "\n",
    "# 访问net层的参数\n",
    "print('访问net层的参数')\n",
    "for name, param in model.net.named_parameters():\n",
    "    print(f\"参数名称：{name}, 形状：{param.shape}\") \n",
    "\n",
    "# 访问linear层的参数\n",
    "print('\\n访问linear层的参数')\n",
    "for name, param in model.linear.named_parameters():\n",
    "    print(f\"参数名称：{name}, 形状：{param.shape}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9811d631-137a-44a7-9046-6d148393d589",
   "metadata": {},
   "source": [
    "### 练习二\n",
    "\n",
    "2. 查看初始化模块文档以了解不同的初始化方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405aadd8-1f05-4a20-a0f2-1e7707f60860",
   "metadata": {},
   "source": [
    "&emsp;&emsp;通过查看深度学习框架文档，有以下初始化方法（参考链接：https://pytorch.org/docs/stable/nn.init.html）\n",
    "- `torch.nn.init.uniform_(tensor, a=0.0, b=1.0)`：以均匀分布$U(a, b)$初始化输入张量。\n",
    "\n",
    "- `torch.nn.init.normal_(tensor, mean=0.0, std=1.0)`：以正态分布$N(mean, std^2)$初始化输入张量。\n",
    "\n",
    "- `torch.nn.init.constant_(tensor, val)`：以一确定数值初始化输入张量。\n",
    "\n",
    "- `torch.nn.init.ones_(tensor)`：用标量值 1 初始化输入张量。\n",
    "\n",
    "- `torch.nn.init.zeros_(tensor)`：用标量值 0 初始化输入张量。\n",
    "\n",
    "- `torch.nn.init.eye_(tensor)`：用单位矩阵初始化二维输入张量。\n",
    "\n",
    "- `torch.nn.init.xavier_uniform_(tensor, gain=1.0)`：从均匀分布$U(-a, a)$中采样，初始化输入张量，其中$a$的值由如下公式确定\n",
    "\n",
    "  $$a= gain * \\sqrt{\\frac{6}{fan_{in}+fan_{out}}}$$\n",
    "\n",
    "  其中$gain$的取值如下表所示\n",
    "<style> table { margin: auto;} </style>\n",
    "非线性函数 | gain值\n",
    ":----:|:----:\n",
    "Linear/Identity | 1\n",
    "Conv1D | 1\n",
    "Conv2D | 1\n",
    "Conv3D | 1\n",
    "Sigmoid | 1\n",
    "Tanh | $\\displaystyle\\frac{5}{3}$\n",
    "ReLU | $\\sqrt{2}$\n",
    "Leaky ReLU | $$\\sqrt{\\frac{2}{1+negative\\_slope^2}}$$\n",
    "SELU | 1 (adaptive)\n",
    "\n",
    "- `torch.nn.init.xavier_normal_(tensor, gain=1.0)`:从正态分布$N(0,std^2)$中采样，初始化输入张量，其中$std$值由下式确定：\n",
    "\n",
    "  $$std= gain * \\sqrt{\\frac{2}{fan_{in}+fan_{out}}}$$\n",
    "\n",
    "- `torch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')`:服从均匀分布$U(−bound, bound)$，其中$bound$值由下式确定\n",
    "\n",
    "  $$bound= gain * \\sqrt{\\frac{3}{fan_{mode}}}$$\n",
    "\n",
    "- `torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')`:服从从正态分布$N(0,std^2)$中采样，其中$std$值由下式确定\n",
    "\n",
    "  $$std= \\frac{gain}{\\sqrt{fan_{mode}}}$$\n",
    "  \n",
    "- `torch.nn.init.trunc_normal_(tensor, mean=0.0, std=1.0, a=- 2.0, b=2.0)`:用从截断的正态分布中提取的值初始化输入张量。这些值实际上是从正态分布 $N(mean, std^2)$中提取的。\n",
    "\n",
    "- `torch.nn.init.sparse_(tensor, sparsity, std=0.01)`：将 2D 输入张量初始化为稀疏矩阵，其中非零元素服从正态分布$N(0,0.01)$。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566b2c29-b9c8-43ed-967d-5e2394ce992d",
   "metadata": {},
   "source": [
    "### 练习三\n",
    "\n",
    "3. 构建包含共享参数层的多层感知机并对其进行训练。在训练过程中，观察模型各层的参数和梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdacbec-7a9a-4c62-9359-eca8f4806177",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在训练过程中，我们每个epoch都打印了每层的参数和梯度。可以看到shared_fc层的参数和梯度都是相同的，因为它们共享同一个参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce23c037-26d7-4360-b55a-8cc4e7766588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "参数名称：0.weight, 具体参数值：tensor([[ 0.5548, -0.3248,  0.2388],\n",
      "        [ 0.1103,  0.0878,  0.4410]]), 参数梯度：tensor([[-0., -0., -0.],\n",
      "        [-0., -0., -0.]])\n",
      "参数名称：0.bias, 具体参数值：tensor([-0.0365, -0.4335]), 参数梯度：tensor([0., 0.])\n",
      "参数名称：2.weight, 具体参数值：tensor([[ 0.4332,  0.2209,  0.3702,  0.3684],\n",
      "        [ 0.2237, -0.4731,  0.3305, -0.1959]]), 参数梯度：tensor([[ 0.1024,  0.6928,  0.0891,  1.2577],\n",
      "        [-0.0705, -0.4772,  0.9290,  0.0946]])\n",
      "参数名称：2.bias, 具体参数值：tensor([-0.3402,  0.3970]), 参数梯度：tensor([1.5441, 0.7866])\n",
      "参数名称：6.weight, 具体参数值：tensor([[-0.1396, -0.3433,  0.0059, -0.2224],\n",
      "        [ 0.2504,  0.2871,  0.2662, -0.0786]]), 参数梯度：tensor([[-1.7733e-01, -3.9085e-02, -4.6856e-01, -1.0457e+00],\n",
      "        [ 2.0208e-03,  4.4541e-04,  5.3397e-03,  1.1917e-02]])\n",
      "参数名称：6.bias, 具体参数值：tensor([-0.0118,  0.3932]), 参数梯度：tensor([-1.2364,  0.0141])\n",
      "Epoch：0, Loss：7.444265842437744\n",
      "\n",
      "Epoch: 1\n",
      "参数名称：0.weight, 具体参数值：tensor([[ 0.5548, -0.3248,  0.2388],\n",
      "        [ 0.1103,  0.0878,  0.4410]]), 参数梯度：tensor([[-0., -0., -0.],\n",
      "        [-0., -0., -0.]])\n",
      "参数名称：0.bias, 具体参数值：tensor([-0.0365, -0.4335]), 参数梯度：tensor([0., 0.])\n",
      "参数名称：2.weight, 具体参数值：tensor([[ 0.4326,  0.2145,  0.3696,  0.3570],\n",
      "        [ 0.2242, -0.4687,  0.3220, -0.1968]]), 参数梯度：tensor([[ 0.0596,  0.6390,  0.0606,  1.1381],\n",
      "        [-0.0418, -0.4480,  0.8527,  0.0939]])\n",
      "参数名称：2.bias, 具体参数值：tensor([-0.3550,  0.3902]), 参数梯度：tensor([1.4834, 0.6797])\n",
      "参数名称：6.weight, 具体参数值：tensor([[-0.1387, -0.3429,  0.0100, -0.2131],\n",
      "        [ 0.2504,  0.2871,  0.2662, -0.0786]]), 参数梯度：tensor([[-8.7157e-02, -3.7479e-02, -4.1491e-01, -9.3261e-01],\n",
      "        [-2.8695e-05, -1.2339e-05, -1.3660e-04, -3.0705e-04]])\n",
      "参数名称：6.bias, 具体参数值：tensor([2.8354e-04, 3.9324e-01]), 参数梯度：tensor([-1.2037e+00, -3.9631e-04])\n",
      "Epoch：1, Loss：7.00782585144043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 模型参数\n",
    "input_size = 3\n",
    "hidden_size = 4\n",
    "output_size = 3\n",
    "lr = 0.01\n",
    "epochs = 2\n",
    "\n",
    "# 构建带有共享参数层的多层感知机\n",
    "shared_fc = nn.Linear(hidden_size, hidden_size)\n",
    "MLP = nn.Sequential(nn.Linear(input_size, hidden_size), nn.ReLU(),\n",
    "                   shared_fc, nn.ReLU(),\n",
    "                   shared_fc, nn.ReLU(),\n",
    "                   nn.Linear(hidden_size, output_size))\n",
    "\n",
    "# 训练数据\n",
    "X = torch.randn(1, input_size)\n",
    "Y = torch.randn(1, output_size)\n",
    "# 优化器\n",
    "optimizer = optim.SGD(MLP.parameters(), lr=lr)\n",
    "# 训练模型\n",
    "for epoch in range(epochs):\n",
    "    # 前向传播和计算损失\n",
    "    Y_pred = MLP(X)\n",
    "    loss = nn.functional.mse_loss(Y_pred, Y)\n",
    "    # 反向传播和更新梯度\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # 打印每层的参数和梯度\n",
    "    print(f'Epoch: {epoch}')\n",
    "    for name, param in MLP.named_parameters():\n",
    "        print(f\"参数名称：{name}, 具体参数值：{param.data[:2]}, 参数梯度：{param.grad[:2]}\") # 为了节省页面空间，这里只打印了每个参数和梯度的前两维\n",
    "    print(f'Epoch：{epoch}, Loss：{loss.item()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f388037-8798-4162-9b3a-c556f7b0956c",
   "metadata": {},
   "source": [
    "可以看出这个过程只有两个共享层的参数和梯度的数值是一样的，我们直接对这两层结果进行进一步的确认。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "747bfdd9-5d1e-4ab1-8fa8-204511772ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数是否相同：tensor([[True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True]])\n",
      "参数是否同时变化：tensor([True, True, True, True])\n",
      "是否是同一个对象：True\n",
      "梯度是否相同：tensor([[True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True]])\n",
      "梯度是否同时变化：tensor([True, True, True, True])\n",
      "是否是同一个对象：True\n"
     ]
    }
   ],
   "source": [
    "# 检查参数是否相同\n",
    "print(f\"参数是否相同：{MLP[2].weight.data == MLP[4].weight.data}\")\n",
    "# 确保它们实际上是同一个对象，而不是有相同的值\n",
    "MLP[2].weight.data[0, 0] = 100\n",
    "print(f\"参数是否同时变化：{MLP[2].weight.data[0] == MLP[4].weight.data[0]}\")\n",
    "print(f\"是否是同一个对象：{MLP[2].weight.data.equal(MLP[4].weight.data)}\")\n",
    "\n",
    "# 检查参数是否相同\n",
    "print(f\"梯度是否相同：{MLP[2].weight.grad == MLP[4].weight.grad}\")\n",
    "# 确保它们实际上是同一个对象，而不是有相同的值\n",
    "MLP[2].weight.grad[0, 0] = 100\n",
    "print(f\"梯度是否同时变化：{MLP[2].weight.grad[0] == MLP[4].weight.grad[0]}\")\n",
    "print(f\"是否是同一个对象：{MLP[2].weight.grad.equal(MLP[4].weight.grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385268a2-f489-4ea2-9117-07f9831e885d",
   "metadata": {},
   "source": [
    "### 练习四\n",
    "\n",
    "4. 为什么共享参数是个好主意？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0e89b5-c5e5-489e-8729-195708c14762",
   "metadata": {},
   "source": [
    "&emsp;&emsp;1. 节约内存：共享参数可以减少模型中需要存储的参数数量，从而减少内存使用。\n",
    "\n",
    "&emsp;&emsp;2. 加速收敛：共享参数可以让模型更加稳定，加速收敛。\n",
    "\n",
    "&emsp;&emsp;3. 提高泛化能力：共享参数可以帮助模型更好地捕捉数据中的共性，提高模型的泛化能力。\n",
    "\n",
    "&emsp;&emsp;4. 加强模型的可解释性：共享参数可以让模型更加简洁明了，加强模型的可解释性。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
