{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d1ed99e-d73e-4d39-8dc6-c31b6d902e7d",
   "metadata": {},
   "source": [
    "# 自定义层\n",
    "\n",
    "深度学习成功背后的一个因素是神经网络的灵活性：\n",
    "我们可以用创造性的方式组合不同的层，从而设计出适用于各种任务的架构。\n",
    "例如，研究人员发明了专门用于处理图像、文本、序列数据和执行动态规划的层。\n",
    "有时我们会遇到或要自己发明一个现在在深度学习框架中还不存在的层。\n",
    "在这些情况下，必须构建自定义层。本节将展示如何构建自定义层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680ac85d-1499-4509-b365-48a2839fe641",
   "metadata": {},
   "source": [
    "## 不带参数的层\n",
    "\n",
    "首先，我们(**构造一个没有任何参数的自定义层**)。\n",
    "回忆一下在[块和层](https://github.com/lixinjie97/Deep_learning_tutorial/blob/main/04.deep-learning-computation/02.model-construction.ipynb)对块的介绍，\n",
    "这应该看起来很眼熟。\n",
    "下面的`CenteredLayer`类要从其输入中减去均值。\n",
    "要构建它，我们只需继承基础层类并实现前向传播功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c65e1b6e-b69e-4a64-a791-881506394ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54051aa7-85e8-4b50-9279-3888a436de74",
   "metadata": {},
   "source": [
    "让我们向该层提供一些数据，验证它是否能按预期工作。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c3ae999-a661-4d02-9b5f-fff4b713e3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = CenteredLayer()\n",
    "layer(torch.FloatTensor([1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3169660d-7da4-4ffa-9234-5468c5be2a6e",
   "metadata": {},
   "source": [
    "现在，我们可以[**将层作为组件合并到更复杂的模型中**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dae58666-2e1b-468c-a25e-d8330f53ed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32482c3-a5be-461c-99a7-65010da699ad",
   "metadata": {},
   "source": [
    "作为额外的健全性检查，我们可以在向该网络发送随机数据后，检查均值是否为0。\n",
    "由于我们处理的是浮点数，因为存储精度的原因，我们仍然可能会看到一个非常小的非零数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "921f520d-1cf3-40aa-b3d5-03c7794a919c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6566e-09, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = net(torch.rand(4, 8))\n",
    "Y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5e9dfc-21f8-4ef6-ad78-dcfc57df12d4",
   "metadata": {},
   "source": [
    "## [**带参数的层**]\n",
    "\n",
    "以上我们知道了如何定义简单的层，下面我们继续定义具有参数的层，\n",
    "这些参数可以通过训练进行调整。\n",
    "我们可以使用内置函数来创建参数，这些函数提供一些基本的管理功能。\n",
    "比如管理访问、初始化、共享、保存和加载模型参数。\n",
    "这样做的好处之一是：我们不需要为每个自定义层编写自定义的序列化程序。\n",
    "\n",
    "现在，让我们实现自定义版本的全连接层。\n",
    "回想一下，该层需要两个参数，一个用于表示权重，另一个用于表示偏置项。\n",
    "在此实现中，我们使用修正线性单元作为激活函数。\n",
    "该层需要输入参数：`in_units`和`units`，分别表示输入数和输出数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68652151-8274-4a88-95bd-9ec0e05e590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_units, units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units, units))\n",
    "        self.bias = nn.Parameter(torch.rand(units,))\n",
    "    def forward(self, X):\n",
    "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
    "        return F.relu(linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7197555c-9bc7-406f-b6ec-b462c500940c",
   "metadata": {},
   "source": [
    "接下来，我们实例化`MyLinear`类并访问其模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a139b8eb-56c3-4fe6-9021-c66b4f4175bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3033, -0.4409, -0.2735],\n",
       "        [-0.7412,  0.1353, -0.2788],\n",
       "        [-1.1309, -0.5983,  0.3643],\n",
       "        [-1.3335, -0.2788,  1.0762],\n",
       "        [-0.1278, -1.0522,  0.9761]], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = MyLinear(5, 3)\n",
    "linear.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d3076f-f173-4818-9ac1-9a9fb399b8a9",
   "metadata": {},
   "source": [
    "我们可以[**使用自定义层直接执行前向传播计算**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ed2049c-4927-4fe8-ae7b-86c6f4c93fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0140, 0.0883, 1.1155],\n",
       "        [0.0000, 0.0000, 2.4038]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(torch.rand(2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2253f0-e649-426c-9842-2e4e940ae727",
   "metadata": {},
   "source": [
    "我们还可以(**使用自定义层构建模型**)，就像使用内置的全连接层一样使用自定义层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "974cad13-eb6e-48b1-84ba-1a93756d1e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.2781],\n",
       "        [5.6565]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))\n",
    "net(torch.rand(2, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d01e47-ef6b-4a00-b063-84d84413f2d1",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 我们可以通过基本层类设计自定义层。这允许我们定义灵活的新层，其行为与深度学习框架中的任何现有层不同。\n",
    "* 在自定义层定义完成后，我们就可以在任意环境和网络架构中调用该自定义层。\n",
    "* 层可以有局部参数，这些参数可以通过内置函数创建。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe1b2b6-b63d-4d8e-9ec8-9c7fcfe35682",
   "metadata": {},
   "source": [
    "## 练习\n",
    "\n",
    "1. 设计一个接受输入并计算张量降维的层，它返回$y_k = \\sum_{i, j} W_{ijk} x_i x_j$。\n",
    "1. 设计一个返回输入数据的傅立叶系数前半部分的层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0f1fc0-0af3-4004-9b50-d6950f589c8b",
   "metadata": {},
   "source": [
    "### 练习一\n",
    "\n",
    "1. 设计一个接受输入并计算张量降维的层，它返回$y_k = \\sum_{i, j} W_{ijk} x_i x_j$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5ee8f7-2ae5-4d19-8e84-29f967fcf2a4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这个公式表示一个线性变换，将输入张量$x$中所有可能的二元组$(x_i,x_j)$进行组合，并对它们进行加权求和。其中，$W_{ijk}$表示权重张量中第$i,j,k$个元素的值。具体而言，该公式计算了输入张量$x$中所有二元组$(x_i, x_j)$对应的特征向量$u_{ij}$：\n",
    "\n",
    "\n",
    "$$\n",
    "u_{ij} = x_i \\cdot x_j\n",
    "$$\n",
    "\n",
    "\n",
    "&emsp;&emsp;然后，根据权重张量$W$中的权重$W_{ijk}$，对所有特征向量$u_{ij}$进行线性组合，得到输出向量$y_k$为：\n",
    "\n",
    "\n",
    "$$\n",
    "y_k = \\sum_{i,j} W_{ijk} u_{ij} = \\sum_{i,j} W_{ijk} x_i x_j\n",
    "$$\n",
    "\n",
    "\n",
    "&emsp;&emsp;该操作可以被视为一种降维操作，将高维输入$x$映射到低维输出空间$y$中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10a3d607-e00a-4c1f-83be-0e54da6c6a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape:torch.Size([2, 10])\n",
      "X:tensor([[0.5466, 0.3143, 0.1361, 0.0980, 0.8118, 0.2218, 0.7450, 0.1392, 0.6839,\n",
      "         0.7425],\n",
      "        [0.9925, 0.1745, 0.3300, 0.8665, 0.8253, 0.4589, 0.4624, 0.0362, 0.7628,\n",
      "         0.8761]])\n",
      "layer(X).shape:torch.Size([2, 5])\n",
      "layer(X):tensor([[ 9.5115, 10.5971,  9.0085, 10.7240,  9.9433],\n",
      "        [16.9628, 16.8431, 15.5624, 17.8134, 17.1410]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TensorReduction(nn.Module):\n",
    "    def __init__(self, dim1, dim2):\n",
    "        super(TensorReduction, self).__init__()\n",
    "        # 定义一个可训练的权重参数，维度为(dim2, dim1, dim1)\n",
    "        self.weight = nn.Parameter(torch.rand(dim2, dim1, dim1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 初始化一个全零张量，大小为(X.shape[0], self.weight.shape[0])\n",
    "        Y = torch.zeros(X.shape[0], self.weight.shape[0])\n",
    "        for k in range(self.weight.shape[0]):\n",
    "            # 计算temp = X @ weight[k] @ X^T\n",
    "            temp = torch.matmul(X, self.weight[k]) @ X.T\n",
    "            # 取temp的对角线元素，存入Y[:, k]\n",
    "            Y[:, k] = temp.diagonal()\n",
    "        return Y\n",
    "\n",
    "# 创建一个TensorReduction层，dim1=10，dim2=5\n",
    "layer = TensorReduction(10, 5)\n",
    "# 创建一个大小为(2, 10)的张量X\n",
    "X = torch.rand(2, 10)\n",
    "# 对layer(X)进行前向传播，返回一个大小为(2, 5)的张量\n",
    "print(f'X.shape:{X.shape}')\n",
    "print(f'X:{X}')\n",
    "print(f'layer(X).shape:{layer(X).shape}')\n",
    "print(f'layer(X):{layer(X)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eed11ff-923a-4fde-9abc-7d333c0ea7ac",
   "metadata": {},
   "source": [
    "把(2, 10)的张量降维到(2, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2f6594-b3cb-4254-86f0-365f9988a876",
   "metadata": {},
   "source": [
    "### 练习二\n",
    "\n",
    "2. 设计一个返回输入数据的傅立叶系数前半部分的层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae266e8-3ce4-479b-a8ab-6d88ae287123",
   "metadata": {},
   "source": [
    "&emsp;&emsp;傅里叶级数可以参考[维基百科](https://en.wikipedia.org/wiki/Fourier_series)中的定义。\n",
    "\n",
    "&emsp;&emsp;在`torch`中有相应的函数可以轻松实现傅里叶级数，如下代码所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bf915af-d6d5-4cb8-a352-301e63d52333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2581, 0.3637, 0.3915, 0.2914, 0.4816],\n",
      "         [0.5522, 0.5383, 0.2517, 0.0116, 0.4663]]])\n",
      "tensor([[[ 3.6066+0.0000j,  0.6166-0.1563j],\n",
      "         [-0.0338+0.0000j, -0.6828+0.2629j]]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.fft as fft\n",
    "\n",
    "class FourierLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FourierLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 对输入的张量 x 进行快速傅里叶变换\n",
    "        x = fft.fftn(x)\n",
    "        # 取出第三个维度的前半部分，即去掉直流分量和镜像分量\n",
    "        x = x[:, :, :x.shape[2] // 2]\n",
    "        # 返回处理后的张量\n",
    "        return x\n",
    "\n",
    "# 创建一个随机数值为 [0, 1) 的形状为 (1, 2, 5) 的张量 X\n",
    "X = torch.rand(1, 2, 5)\n",
    "# 实例化一个 FourierLayer 的网络对象 net\n",
    "net = FourierLayer()\n",
    "# 将 X 输入到网络 net 中进行前向计算，并输出结果\n",
    "print(X)\n",
    "print(net(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
