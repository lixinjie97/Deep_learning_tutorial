{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637fd2da-44c9-49fc-95b0-37d456bbc6ec",
   "metadata": {},
   "source": [
    "# 读写文件\n",
    "\n",
    "到目前为止，我们讨论了如何处理数据，\n",
    "以及如何构建、训练和测试深度学习模型。\n",
    "然而，有时我们希望保存训练的模型，\n",
    "以备将来在各种环境中使用（比如在部署中进行预测）。\n",
    "此外，当运行一个耗时较长的训练过程时，\n",
    "最佳的做法是定期保存中间结果，\n",
    "以确保在服务器电源被不小心断掉时，我们不会损失几天的计算结果。\n",
    "因此，现在是时候学习如何加载和存储权重向量和整个模型了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e47d1f-ba97-4b6f-bb59-82dc52dca87a",
   "metadata": {},
   "source": [
    "## (**加载和保存张量**)\n",
    "\n",
    "对于单个张量，我们可以直接调用`load`和`save`函数分别读写它们。\n",
    "这两个函数都要求我们提供一个名称，`save`要求将要保存的变量作为输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5ffaf1a-61b7-4199-9eaa-bb0e1d975799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "x = torch.arange(4)\n",
    "torch.save(x, 'x-file')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c3b5ef-d6f7-4f14-9216-559ddcb8e37e",
   "metadata": {},
   "source": [
    "我们现在可以将存储在文件中的数据读回内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99896573-f66d-48e0-b67e-4bef1632630f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = torch.load('x-file')\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8805a0fc-2bd6-4cad-ad72-d8ae2d877318",
   "metadata": {},
   "source": [
    "我们可以[**存储一个张量列表，然后把它们读回内存。**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66b9e0a6-2f77-4e91-b8fa-e43c32f54ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros(4)\n",
    "torch.save([x, y], 'x-files')\n",
    "x2, y2 = torch.load('x-files')\n",
    "(x2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbf0794-056b-4a27-b0d7-a0d67d118b75",
   "metadata": {},
   "source": [
    "我们甚至可以(**写入或读取从字符串映射到张量的字典**)。\n",
    "当我们要读取或写入模型中的所有权重时，这很方便。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56c5cb6b-4147-4f8a-97d6-e7fd1b5931c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict = {'x': x, 'y': y}\n",
    "torch.save(mydict, 'mydict')\n",
    "mydict2 = torch.load('mydict')\n",
    "mydict2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c43104-2b6e-4bba-9cb7-2118e0d88206",
   "metadata": {},
   "source": [
    "## [**加载和保存模型参数**]\n",
    "\n",
    "保存单个权重向量（或其他张量）确实有用，\n",
    "但是如果我们想保存整个模型，并在以后加载它们，\n",
    "单独保存每个向量则会变得很麻烦。\n",
    "毕竟，我们可能有数百个参数散布在各处。\n",
    "因此，深度学习框架提供了内置函数来保存和加载整个网络。\n",
    "**需要注意的一个重要细节是，这将保存模型的参数而不是保存整个模型。**\n",
    "例如，如果我们有一个3层多层感知机，我们需要单独指定架构。\n",
    "因为模型本身可以包含任意代码，所以模型本身难以序列化。\n",
    "因此，为了恢复模型，我们需要用代码生成架构，\n",
    "然后从磁盘加载参数。\n",
    "让我们从熟悉的多层感知机开始尝试一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6f26466-dc2b-43b5-a625-87cbc1d380c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3.5486e-01,  2.2585e+00, -1.1827e+00, -5.5131e-01, -7.9360e-01,\n",
       "           1.0321e+00, -1.8008e+00,  7.8140e-01,  3.7105e-01, -9.8850e-01,\n",
       "          -5.2159e-01, -6.5740e-01,  7.7749e-02, -1.0196e+00, -1.1168e+00,\n",
       "           1.5408e+00,  6.3659e-01,  7.8005e-02, -7.2309e-01,  9.7826e-01],\n",
       "         [ 4.2258e-01, -1.6792e+00,  5.7807e-01,  1.2039e+00, -5.1406e-02,\n",
       "           8.3716e-01,  1.6647e-01,  3.4777e-01, -5.6235e-01,  3.2793e-01,\n",
       "           5.4545e-02, -2.5604e-01,  7.1732e-01, -1.3734e+00, -2.1747e-01,\n",
       "           1.3123e-01,  1.2164e+00,  8.7483e-01, -5.1444e-01, -6.6577e-04]]),\n",
       " tensor([[-0.1264,  0.0921, -0.3392, -0.2413,  0.1113, -0.2561,  0.3629,  0.5479,\n",
       "           0.0899, -0.4099],\n",
       "         [-0.1601, -0.0084, -0.2603, -0.2037,  0.1824, -0.1611,  0.1112,  0.1003,\n",
       "           0.1286, -0.3214]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.output = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn(size=(2, 20))\n",
    "Y = net(X)\n",
    "X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4603962-a4e0-4cf2-ad0f-beff9ad2956d",
   "metadata": {},
   "source": [
    "接下来，我们[**将模型的参数存储在一个叫做“mlp.params”的文件中。**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "607d1787-b4e8-4ea4-bac3-ea31f7bc3aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hidden.weight',\n",
       "              tensor([[-0.1332,  0.0322, -0.1505,  ..., -0.0882,  0.0452,  0.0668],\n",
       "                      [-0.1153, -0.1790,  0.0884,  ...,  0.1508,  0.1164, -0.1673],\n",
       "                      [ 0.0049,  0.1845,  0.0719,  ..., -0.2096,  0.1095,  0.1662],\n",
       "                      ...,\n",
       "                      [-0.1599,  0.0469, -0.0083,  ...,  0.0888, -0.0866,  0.1037],\n",
       "                      [-0.1732,  0.1461,  0.0695,  ..., -0.0616, -0.0981, -0.0066],\n",
       "                      [-0.2098, -0.0056,  0.0070,  ...,  0.0902,  0.1029,  0.1930]])),\n",
       "             ('hidden.bias',\n",
       "              tensor([ 0.0214,  0.1554,  0.0400, -0.2069, -0.0713,  0.0069,  0.1689,  0.1613,\n",
       "                       0.1658, -0.1383, -0.0152, -0.2150,  0.0255,  0.1440, -0.2120, -0.0483,\n",
       "                      -0.1886, -0.1127, -0.1309,  0.0746, -0.0862,  0.1152, -0.0483,  0.0302,\n",
       "                       0.0844, -0.1584, -0.1108, -0.1183,  0.1327, -0.1274,  0.1651,  0.2010,\n",
       "                      -0.0992,  0.2185, -0.0783,  0.1431, -0.2017,  0.0986,  0.0530, -0.0423,\n",
       "                       0.1671,  0.1666, -0.0561,  0.1674, -0.0221, -0.0018,  0.1080, -0.0039,\n",
       "                       0.0650,  0.0934,  0.2160, -0.0607, -0.1481,  0.0968,  0.2017, -0.1703,\n",
       "                       0.0621, -0.2125,  0.1493,  0.0777, -0.2184,  0.1127,  0.2003,  0.0181,\n",
       "                       0.1477, -0.0135, -0.1227, -0.1509,  0.1243,  0.0282,  0.0683, -0.1355,\n",
       "                      -0.0063,  0.0177, -0.1249,  0.1590,  0.1712, -0.0264,  0.2066, -0.0318,\n",
       "                      -0.1345, -0.1781,  0.0009,  0.0364,  0.0870,  0.1991, -0.1696,  0.0332,\n",
       "                      -0.0783,  0.1328,  0.1438,  0.0293,  0.0215,  0.1178,  0.0829,  0.1069,\n",
       "                      -0.2234, -0.1044, -0.1864,  0.1437, -0.0081,  0.0003,  0.0882,  0.0211,\n",
       "                      -0.1577, -0.0153, -0.0097, -0.1716,  0.1923, -0.0244,  0.2131, -0.2205,\n",
       "                      -0.0151, -0.0249, -0.0402, -0.1586,  0.1883, -0.1731,  0.2220, -0.1762,\n",
       "                      -0.1594,  0.1932, -0.0997,  0.0651,  0.1139,  0.1180, -0.0667, -0.0321,\n",
       "                      -0.0968, -0.2072,  0.0539, -0.0100,  0.1969,  0.0078,  0.1845, -0.0583,\n",
       "                       0.1842, -0.0460,  0.0473, -0.0427, -0.0750, -0.1482,  0.0310, -0.1791,\n",
       "                       0.0064,  0.2210,  0.2155,  0.1304, -0.1647,  0.0600, -0.0727, -0.0159,\n",
       "                       0.0131, -0.0948,  0.2002,  0.0977, -0.0598, -0.0948,  0.0641,  0.1433,\n",
       "                      -0.2033,  0.0195,  0.0419,  0.1257,  0.1685, -0.0015, -0.0014, -0.0415,\n",
       "                       0.0553, -0.0135,  0.1709,  0.0879, -0.0033,  0.0774,  0.0201,  0.1946,\n",
       "                      -0.0142,  0.0237, -0.1746, -0.0728,  0.0550, -0.1622, -0.1553, -0.1042,\n",
       "                       0.0598,  0.0307,  0.2140,  0.1241,  0.2136,  0.1264,  0.0991, -0.0366,\n",
       "                      -0.0612,  0.0384,  0.1004,  0.0204,  0.0715, -0.0205,  0.0248,  0.2096,\n",
       "                       0.0206, -0.1864,  0.2169, -0.0665,  0.1618,  0.1630, -0.1042,  0.0609,\n",
       "                       0.1474, -0.0187, -0.1725, -0.0211,  0.0413, -0.1048,  0.0498,  0.1278,\n",
       "                       0.0638, -0.0630,  0.2204,  0.0702, -0.0697,  0.0843,  0.1152,  0.0343,\n",
       "                      -0.2110, -0.1125,  0.1007,  0.1164,  0.1166, -0.1819,  0.0095,  0.0692,\n",
       "                      -0.1092, -0.1351, -0.2194, -0.1025, -0.1009, -0.0030, -0.0517,  0.1793,\n",
       "                      -0.0746,  0.0908, -0.1627,  0.0692,  0.1238, -0.0697,  0.0867, -0.0331,\n",
       "                      -0.1979,  0.1971, -0.0725, -0.0274, -0.0788, -0.1338, -0.2196,  0.0611])),\n",
       "             ('output.weight',\n",
       "              tensor([[ 0.0480,  0.0461, -0.0169,  ..., -0.0449,  0.0275, -0.0228],\n",
       "                      [-0.0499, -0.0460,  0.0368,  ..., -0.0144,  0.0265,  0.0080],\n",
       "                      [ 0.0526,  0.0605,  0.0173,  ..., -0.0318, -0.0231,  0.0505],\n",
       "                      ...,\n",
       "                      [ 0.0207, -0.0384,  0.0153,  ...,  0.0170, -0.0241,  0.0027],\n",
       "                      [-0.0016,  0.0315, -0.0140,  ..., -0.0338, -0.0179,  0.0590],\n",
       "                      [ 0.0058, -0.0149, -0.0355,  ..., -0.0377, -0.0492,  0.0055]])),\n",
       "             ('output.bias',\n",
       "              tensor([-0.0123, -0.0131, -0.0539,  0.0387, -0.0234,  0.0600, -0.0129,  0.0525,\n",
       "                       0.0543, -0.0308]))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d991cc49-776e-454e-b7a9-f0edaa0b2767",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'mlp.params')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025c1856-79c5-4709-a4bc-5e146ddab6fa",
   "metadata": {},
   "source": [
    "为了恢复模型，我们[**实例化了原始多层感知机模型的一个备份。**]\n",
    "这里我们不需要随机初始化模型参数，而是(**直接读取文件中存储的参数。**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9c54a0b-0d42-4100-ab6e-fbbfefc3739a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load('mlp.params'))\n",
    "clone.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830b0a47-f972-4eb7-93e5-36b144547c22",
   "metadata": {},
   "source": [
    "由于两个实例具有相同的模型参数，在输入相同的`X`时，\n",
    "两个实例的计算结果应该相同。\n",
    "让我们来验证一下。\n",
    "\n",
    "注：eval评估模式下dropout会被关掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e61a52d2-7a21-4083-9abb-a809fe98d6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3073da-21bc-49fa-adf7-d1afc3c6467f",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* `save`和`load`函数可用于张量对象的文件读写。\n",
    "* 我们可以通过参数字典保存和加载网络的全部参数。\n",
    "* 保存架构必须在代码中完成，而不是在参数中完成。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dee652-2412-4a6e-a466-c1c087a72b01",
   "metadata": {},
   "source": [
    "## 练习\n",
    "\n",
    "1. 即使不需要将经过训练的模型部署到不同的设备上，存储模型参数还有什么实际的好处？\n",
    "1. 假设我们只想复用网络的一部分，以将其合并到不同的网络架构中。比如想在一个新的网络中使用之前网络的前两层，该怎么做？\n",
    "1. 如何同时保存网络架构和参数？需要对架构加上什么限制？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
