{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7df11ba-1284-4331-a806-6cba308cc5e6",
   "metadata": {},
   "source": [
    "# 图像卷积\n",
    "\n",
    "上节我们解析了卷积层的原理，现在我们看看它的实际应用。由于卷积神经网络的设计是用于探索图像数据，本节我们将以图像为例。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6467f0fb-e43e-43dd-84fa-6ee6c4e6fdd4",
   "metadata": {},
   "source": [
    "## 互相关运算\n",
    "\n",
    "严格来说，卷积层是个错误的叫法，因为它所表达的运算其实是*互相关运算*（cross-correlation），而不是卷积运算。\n",
    "根据 [从全连接层到卷积](https://github.com/lixinjie97/Deep_learning_tutorial/blob/main/05.convolutional-neural-networks/02.why-conv.ipynb)中的描述，在卷积层中，输入张量和核张量通过(**互相关运算**)产生输出张量。\n",
    "\n",
    "首先，我们暂时忽略通道（第三维）这一情况，看看如何处理二维图像数据和隐藏表示。在下图中，输入是高度为$3$、宽度为$3$的二维张量（即形状为$3 \\times 3$）。卷积核的高度和宽度都是$2$，而卷积核窗口（或卷积窗口）的形状由内核的高度和宽度决定（即$2 \\times 2$）。\n",
    "\n",
    "![二维互相关运算。阴影部分是第一个输出元素，以及用于计算输出的输入张量元素和核张量元素：$0\\times0+1\\times1+3\\times2+4\\times3=19$.](../assets/correlation.svg)\n",
    "\n",
    "在二维互相关运算中，卷积窗口从输入张量的左上角开始，从左到右、从上到下滑动。\n",
    "当卷积窗口滑动到新一个位置时，包含在该窗口中的部分张量与卷积核张量进行按元素相乘，得到的张量再求和得到一个单一的标量值，由此我们得出了这一位置的输出张量值。\n",
    "在如上例子中，输出张量的四个元素由二维互相关运算得到，这个输出高度为$2$、宽度为$2$，如下所示：\n",
    "\n",
    "$$\n",
    "0\\times0+1\\times1+3\\times2+4\\times3=19,\\\\\n",
    "1\\times0+2\\times1+4\\times2+5\\times3=25,\\\\\n",
    "3\\times0+4\\times1+6\\times2+7\\times3=37,\\\\\n",
    "4\\times0+5\\times1+7\\times2+8\\times3=43.\n",
    "$$\n",
    "\n",
    "注意，输出大小略小于输入大小。这是因为卷积核的宽度和高度大于1，\n",
    "而卷积核只与图像中每个大小完全适合的位置进行互相关运算。\n",
    "所以，输出大小等于输入大小$n_h \\times n_w$减去卷积核大小$k_h \\times k_w$，即：\n",
    "\n",
    "$$(n_h-k_h+1) \\times (n_w-k_w+1).$$\n",
    "\n",
    "这是因为我们需要足够的空间在图像上“移动”卷积核。稍后，我们将看到如何通过在图像边界周围填充零来保证有足够的空间移动卷积核，从而保持输出大小不变。\n",
    "接下来，我们在`corr2d`函数中实现如上过程，该函数接受输入张量`X`和卷积核张量`K`，并返回输出张量`Y`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adf537f3-d7b2-476d-b646-5b723a59fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f188dc9f-9ebc-45c4-addf-777ddd1cebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d(X, k):\n",
    "    \"\"\"计算二维互相关运算\"\"\"\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i+h, j:j+w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4319c71d-f2a0-473c-9d67-452b6a158e03",
   "metadata": {},
   "source": [
    "通过上图的输入张量`X`和卷积核张量`K`，我们来[**验证上述二维互相关运算的输出**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca072d0a-3ce0-4617-a158-16aff8aff1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aac6bd6-2df7-4360-8ce9-39d8e42ebc25",
   "metadata": {},
   "source": [
    "## 卷积层\n",
    "\n",
    "卷积层对输入和卷积核权重进行互相关运算，并在添加标量偏置之后产生输出。\n",
    "所以，卷积层中的两个被训练的参数是卷积核权重和标量偏置。\n",
    "就像我们之前随机初始化全连接层一样，在训练基于卷积层的模型时，我们也随机初始化卷积核权重。\n",
    "\n",
    "基于上面定义的`corr2d`函数[**实现二维卷积层**]。在`__init__`构造函数中，将`weight`和`bias`声明为两个模型参数。前向传播函数调用`corr2d`函数并添加偏置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66ecc4df-82e7-40f8-bffc-a4b3fc1e6e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32e84f8-ff9c-4694-b7a9-792a6872266e",
   "metadata": {},
   "source": [
    "高度和宽度分别为$h$和$w$的卷积核可以被称为$h \\times w$卷积或$h \\times w$卷积核。\n",
    "我们也将带有$h \\times w$卷积核的卷积层称为$h \\times w$卷积层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ba61ee-b0cf-4c32-949b-50fcf4026332",
   "metadata": {},
   "source": [
    "## 图像中目标的边缘检测\n",
    "\n",
    "如下是[**卷积层的一个简单应用：**]通过找到像素变化的位置，来(**检测图像中不同颜色的边缘**)。\n",
    "首先，我们构造一个$6\\times 8$像素的黑白图像。中间四列为黑色（$0$），其余像素为白色（$1$）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d8d823f-d870-49c8-a160-2d615a40875b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5891a52-72dd-4f9d-b792-966bc5dfb111",
   "metadata": {},
   "source": [
    "接下来，我们构造一个高度为$1$、宽度为$2$的卷积核`K`。当进行互相关运算时，如果水平相邻的两元素相同，则输出为零，否则输出为非零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "507236a4-8284-4c52-a167-c906ca342771",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = torch.tensor([[1.0, -1.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad0729-e0d4-4cac-b43e-68981cbf3712",
   "metadata": {},
   "source": [
    "现在，我们对参数`X`（输入）和`K`（卷积核）执行互相关运算。\n",
    "如下所示，[**输出`Y`中的1代表从白色到黑色的边缘，-1代表从黑色到白色的边缘**]，其他情况的输出为$0$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acc83b43-55d3-4a10-94f5-3ba4c42485f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed0fde-babc-4f19-9fb5-475473b5de31",
   "metadata": {},
   "source": [
    "现在我们将输入的二维图像转置，再进行如上的互相关运算。\n",
    "其输出如下，之前检测到的垂直边缘消失了。\n",
    "不出所料，这个[**卷积核`K`只可以检测垂直边缘**]，无法检测水平边缘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88cbdeee-cdf9-4b9f-98c1-5a2f8d4671ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X.t(), K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26a8c1f-0baf-4cad-8537-4b72546966a8",
   "metadata": {},
   "source": [
    "## 学习卷积核\n",
    "\n",
    "如果我们只需寻找黑白边缘，那么以上`[1, -1]`的边缘检测器足以。然而，当有了更复杂数值的卷积核，或者连续的卷积层时，我们不可能手动设计滤波器。那么我们是否可以[**学习由`X`生成`Y`的卷积核**]呢？\n",
    "\n",
    "现在让我们看看是否可以通过仅查看“输入-输出”对来学习由`X`生成`Y`的卷积核。\n",
    "我们先构造一个卷积层，并将其卷积核初始化为随机张量。接下来，在每次迭代中，我们比较`Y`与卷积层输出的平方误差，然后计算梯度来更新卷积核。为了简单起见，我们在此使用内置的二维卷积层，并忽略偏置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e975e962-758d-47e7-9c64-d319acaa1759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 4.966\n",
      "epoch 4, loss 0.960\n",
      "epoch 6, loss 0.213\n",
      "epoch 8, loss 0.057\n",
      "epoch 10, loss 0.018\n"
     ]
    }
   ],
   "source": [
    "# 构造一个二维卷积层，它具有1个输出通道和形状为(1, 2)的卷积核\n",
    "\n",
    "conv2d = nn.Conv2d(1, 1, kernel_size=(1, 2), bias=False)\n",
    "\n",
    "# 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度）\n",
    "# 其中批量大小和通道数都为1\n",
    "X = X.reshape((1, 1, 6, 8))\n",
    "Y = Y.reshape((1, 1, 6, 7))\n",
    "lr = 3e-2 # 学习率\n",
    "\n",
    "for i in range(10):\n",
    "    Y_hat = conv2d(X)\n",
    "    l = (Y_hat - Y) ** 2\n",
    "    conv2d.zero_grad()\n",
    "    l.sum().backward()\n",
    "    # 迭代卷积核\n",
    "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print(f'epoch {i+1}, loss {l.sum():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9f3082-f9b5-45b6-a0a3-7f7fa152839f",
   "metadata": {},
   "source": [
    "在$10$次迭代之后，误差已经降到足够低。现在我们来看看我们[**所学的卷积核的权重张量**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2c262f9-0040-47b2-b426-b675956d7611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0016, -0.9765]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.weight.data.reshape((1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b8bb8a-7a41-49af-9709-1cfa589bf9e2",
   "metadata": {},
   "source": [
    "细心的读者一定会发现，我们学习到的卷积核权重非常接近我们之前定义的卷积核`K`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145bf63b-3bc1-45d1-a9e9-8bc73642ccc7",
   "metadata": {},
   "source": [
    "## 互相关和卷积\n",
    "\n",
    "回想一下我们在[从全连接层到卷积](https://github.com/lixinjie97/Deep_learning_tutorial/blob/main/05.convolutional-neural-networks/02.why-conv.ipynb)中观察到的互相关和卷积运算之间的对应关系。\n",
    "**为了得到正式的*卷积*运算输出，我们需要执行 :eqref:`eq_2d-conv-discrete`中定义的严格卷积运算，而不是互相关运算。**\n",
    "幸运的是，它们差别不大，**我们只需水平和垂直翻转二维卷积核张量，然后对输入张量执行*互相关*运算**。\n",
    "\n",
    "值得注意的是，由于卷积核是从数据中学习到的，因此无论这些层执行严格的卷积运算还是互相关运算，卷积层的输出都不会受到影响。\n",
    "为了说明这一点，假设卷积层执行*互相关*运算并学习上图中的卷积核，该卷积核在这里由矩阵$\\mathbf{K}$表示。\n",
    "假设其他条件不变，当这个层执行严格的*卷积*时，学习的卷积核$\\mathbf{K}'$在水平和垂直翻转之后将与$\\mathbf{K}$相同。\n",
    "也就是说，当卷积层对上图中的输入和$\\mathbf{K}'$执行严格*卷积*运算时，将得到与互相关运算上图中相同的输出。\n",
    "\n",
    "为了与深度学习文献中的标准术语保持一致，我们将继续把“互相关运算”称为卷积运算，尽管严格地说，它们略有不同。\n",
    "此外，对于卷积核张量上的权重，我们称其为*元素*。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec21920-214c-40ef-8ce7-a26e880bb1ec",
   "metadata": {},
   "source": [
    "## 特征映射和感受野\n",
    "\n",
    "如在[从全连接层到卷积一节讲的通道](https://github.com/lixinjie97/Deep_learning_tutorial/blob/main/05.convolutional-neural-networks/02.why-conv.ipynb)中所述， 上图中输出的卷积层有时被称为*特征映射*（feature map），因为它可以被视为一个输入映射到下一层的空间维度的转换器。\n",
    "在卷积神经网络中，对于某一层的任意元素$x$，其*感受野*（receptive field）是指在前向传播期间可能影响$x$计算的所有元素（来自所有先前层）。\n",
    "\n",
    "请注意，感受野可能大于输入的实际大小。让我们用上图为例来解释感受野：\n",
    "给定$2 \\times 2$卷积核，阴影输出元素值$19$的感受野是输入阴影部分的四个元素。\n",
    "假设之前输出为$\\mathbf{Y}$，其大小为$2 \\times 2$，现在我们在其后附加一个卷积层，该卷积层以$\\mathbf{Y}$为输入，输出单个元素$z$。\n",
    "在这种情况下，$\\mathbf{Y}$上的$z$的感受野包括$\\mathbf{Y}$的所有四个元素，而输入的感受野包括最初所有九个输入元素。\n",
    "因此，当一个特征图中的任意元素需要检测更广区域的输入特征时，我们可以构建一个更深的网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37eeaea-f55c-47b5-972c-382c4fca11af",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 二维卷积层的核心计算是二维互相关运算。最简单的形式是，对二维输入数据和卷积核执行互相关操作，然后添加一个偏置。\n",
    "* 我们可以设计一个卷积核来检测图像的边缘。\n",
    "* 我们可以从数据中学习卷积核的参数。\n",
    "* 学习卷积核时，无论用严格卷积运算或互相关运算，卷积层的输出不会受太大影响。\n",
    "* 当需要检测输入特征中更广区域时，我们可以构建一个更深的卷积网络。\n",
    "* 卷积运算和互相关运算差别不大，我们只需要水平和垂直翻转二维卷积核张量，然后对输入张量执行互相关运算得到的输出就是严格的卷积运算的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a14d696-f895-49f7-8e8a-45556aca2ead",
   "metadata": {},
   "source": [
    "## 练习\n",
    "\n",
    "1. 构建一个具有对角线边缘的图像`X`。\n",
    "    1. 如果将本节中举例的卷积核`K`应用于`X`，会发生什么情况？\n",
    "    1. 如果转置`X`会发生什么？\n",
    "    1. 如果转置`K`会发生什么？\n",
    "1. 在我们创建的`Conv2D`自动求导时，有什么错误消息？\n",
    "1. 如何通过改变输入张量和卷积核张量，将互相关运算表示为矩阵乘法？\n",
    "1. 手工设计一些卷积核。\n",
    "    1. 二阶导数的核的形式是什么？\n",
    "    1. 积分的核的形式是什么？\n",
    "    1. 得到$d$次导数的最小核的大小是多少？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc996d2-9f35-4142-9180-eb215a1d193c",
   "metadata": {},
   "source": [
    "### 练习一\n",
    "\n",
    "1. 构建一个具有对角线边缘的图像`X`。\n",
    "   1. 如果将本节中举例的卷积核`K`应用于`X`，会发生什么情况？\n",
    "    1. 如果转置`X`会发生什么？\n",
    "    1. 如果转置`K`会发生什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eeeea0-38b0-45af-b95f-3b5a95b92642",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024f6885-1642-4421-9e43-68bda8ad28c8",
   "metadata": {},
   "source": [
    "**第1问：**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95214610-7ea2-4a60-8901-e9f24f477628",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在对角线处分别为 $1$ 和 $-1$ 的数据，其他区域都为 $0$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fb5cb33-37ef-4ca7-ba48-6ee51a4b4890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1.]])\n",
      "tensor([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., -1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., -1.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0., -1.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., -1.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., -1.]])\n"
     ]
    }
   ],
   "source": [
    "# 代码验证\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "def corr2d(X, K):\n",
    "    \"\"\"计算二维互相关运算\"\"\"\n",
    "    h, w = K.shape # 获取卷积核的高度和宽度\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)) # 初始化输出矩阵 Y，大小为 (X行数 - K行数 + 1) x (X列数 - K列数 + 1)\n",
    "    for i in range(Y.shape[0]): # 遍历 Y 的列数\n",
    "        for j in range(Y.shape[1]): # 遍历 Y 的列数\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum() # 在 Y 的每个位置，计算 X 与 K 的互相关运算结果\n",
    "    return Y\n",
    "\n",
    "# 创建一个8x8的单位矩阵 X\n",
    "X = torch.eye(8)\n",
    "print(X)\n",
    "# 创建一个1x2的卷积核 K，该卷积核在X上进行互相关运算\n",
    "K = torch.tensor([[1.0, -1.0]])\n",
    "# 调用corr2d函数，计算互相关运算结果Y\n",
    "Y = corr2d(X, K)\n",
    "# 打印互相关运算结果Y\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e85cf2d-fe88-40f5-a20e-f4dc88623a3a",
   "metadata": {},
   "source": [
    "**第2问：**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f011052-1afb-4905-8a7c-e6949e414de1",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`X`转置后，结果不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04c47594-a286-4852-a46f-0a825c083be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1.]])\n",
      "tensor([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., -1.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0., -1.,  1.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., -1.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0., -1.,  1.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., -1.,  1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., -1.]])\n"
     ]
    }
   ],
   "source": [
    "# 代码验证\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "def corr2d(X, K):\n",
    "    \"\"\"计算二维互相关运算\"\"\"\n",
    "    h, w = K.shape # 获取卷积核的高度和宽度\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)) # 初始化输出矩阵 Y，大小为 (X行数 - K行数 + 1) x (X列数 - K列数 + 1)\n",
    "    for i in range(Y.shape[0]): # 遍历 Y 的列数\n",
    "        for j in range(Y.shape[1]): # 遍历 Y 的列数\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum() # 在 Y 的每个位置，计算 X 与 K 的互相关运算结果\n",
    "    return Y\n",
    "\n",
    "# 创建一个8x8的单位矩阵 X\n",
    "X = torch.eye(8)\n",
    "print(X.T)\n",
    "# 创建一个1x2的卷积核 K，该卷积核在X上进行互相关运算\n",
    "K = torch.tensor([[1.0, -1.0]])\n",
    "# 调用corr2d函数，计算互相关运算结果Y\n",
    "Y = corr2d(X.T, K) # 在 X 的转置上应用卷积核 K，得到卷积响应图像 Y\n",
    "# 打印互相关运算结果Y\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e90c10-8a52-49e4-a794-32d949ba653e",
   "metadata": {},
   "source": [
    "**第3问：**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ce62de-7686-478f-ad06-4b270af528cc",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`K`转置后，结果也会发生转置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea0e009a-23d6-4a3c-930a-6609079f8a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1.]])\n",
      "tensor([[ 1.],\n",
      "        [-1.]])\n",
      "tensor([[ 1., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1., -1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1., -1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  1., -1.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  1., -1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  1., -1.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  1., -1.]])\n"
     ]
    }
   ],
   "source": [
    "# 代码验证\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "def corr2d(X, K):\n",
    "    \"\"\"计算二维互相关运算\"\"\"\n",
    "    h, w = K.shape # 获取卷积核的高度和宽度\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)) # 初始化输出矩阵 Y，大小为 (X行数 - K行数 + 1) x (X列数 - K列数 + 1)\n",
    "    for i in range(Y.shape[0]): # 遍历 Y 的列数\n",
    "        for j in range(Y.shape[1]): # 遍历 Y 的列数\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum() # 在 Y 的每个位置，计算 X 与 K 的互相关运算结果\n",
    "    return Y\n",
    "\n",
    "# 创建一个8x8的单位矩阵 X\n",
    "X = torch.eye(8)\n",
    "print(X)\n",
    "# 创建一个1x2的卷积核 K，该卷积核在X上进行互相关运算\n",
    "K = torch.tensor([[1.0, -1.0]])\n",
    "print(K.T)\n",
    "# 调用corr2d函数，计算互相关运算结果Y\n",
    "Y = corr2d(X, K.T) # 在 X 应用卷积核 K 的转置，得到卷积响应图像 Y\n",
    "# 打印互相关运算结果Y\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1212ee-ec43-44a9-9a21-66401ba05168",
   "metadata": {},
   "source": [
    "### 练习二\n",
    "\n",
    "2. 在我们创建的`Conv2D`自动求导时，有什么错误消息？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85c1626-a737-4e3d-8fed-39ca92bee034",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f63113f-0cc5-4b29-be32-4ba877c053ee",
   "metadata": {},
   "source": [
    "&emsp;&emsp;会提示维度不对称的错误信息，因为`torch`提供的二维卷积层是`nn.Conv2d()`采用的是四维输入和输出格式（批量大小、通道、高度、宽度），而我们自定义的仅仅是二维的。\n",
    "\n",
    "&emsp;&emsp;使用`nn.Conv2d()`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61824291-7653-4365-aa13-e62609572978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 3.682\n",
      "epoch 4, loss 0.925\n",
      "epoch 6, loss 0.281\n",
      "epoch 8, loss 0.099\n",
      "epoch 10, loss 0.038\n"
     ]
    }
   ],
   "source": [
    "# 代码验证\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "# 定义二维互相关运算函数\n",
    "def corr2d(X, K):\n",
    "    \"\"\"计算二维互相关运算\"\"\"\n",
    "    h, w = K.shape # 获取卷积核的高度和宽度\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)) # 初始化输出矩阵 Y\n",
    "    for i in range(Y.shape[0]): # 遍历输出矩阵的每一行\n",
    "        for j in range(Y.shape[1]): # 遍历输出矩阵的每一列\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum() # 执行互相关运算并赋值给Y的相应位置\n",
    "    return Y\n",
    "\n",
    "# 定义一个二维卷积层类\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__() # 调用nn.Module的构造函数\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size)) # 初始化卷积参数\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x): # 定义前向传播函数\n",
    "        return corr2d(x, self.weight) + self.bias # 返回卷积运算结果\n",
    "\n",
    "# 创建一个6x8的矩阵，中间4列为0\n",
    "X = d2l.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "\n",
    "# 创建一个1x2的卷积核\n",
    "K = d2l.tensor([[1.0, -1.0]])\n",
    "\n",
    "# 进行卷积运算\n",
    "Y = corr2d(X, K)\n",
    "\n",
    "# 对X进行转置，然后再次进行卷积运算\n",
    "corr2d(d2l.transpose(X), K)\n",
    "\n",
    "# 创建一个卷积层，具有1个输出通道和（1, 2）形状的卷积核\n",
    "conv2d = nn.Conv2d(1, 1, kernel_size=(1, 2), bias=False)\n",
    "\n",
    "\n",
    "# 调整X和Y的形状以符合PyTorch卷积层的输入输出要求\n",
    "X = X.reshape((1, 1, 6, 8))\n",
    "Y = Y.reshape((1, 1, 6, 7))\n",
    "\n",
    "# 设置学习率\n",
    "lr = 3e-2\n",
    "\n",
    "# 尝试进行10次迭代\n",
    "try:\n",
    "    for i in range(10):\n",
    "        Y_hat = conv2d(X) # 前向传播\n",
    "        l = (Y_hat - Y) ** 2 # 计算损失\n",
    "        conv2d.zero_grad() # 梯度消失\n",
    "        l.sum().backward() # 反向传播\n",
    "        conv2d.weight.data[:] -= lr * conv2d.weight.grad # 更新权重\n",
    "        if (i + 1) % 2 == 0: # 每两个epoch打印一次损失值\n",
    "            print(f'epoch {i+1}, loss {l.sum():.3f}')\n",
    "except Exception as e: # 捕获并打印任何异常\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6658d6b2-c844-4919-a8e0-345a5c80de22",
   "metadata": {},
   "source": [
    "&emsp;&emsp;使用创建的`Conv2D`时可能会报如下错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbe0fdb0-e040-442e-a1c3-e2c289ff6007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of tensor a (0) must match the size of tensor b (7) at non-singleton dimension 3\n"
     ]
    }
   ],
   "source": [
    "# 代码验证\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "# 定义二维互相关运算函数\n",
    "def corr2d(X, K):\n",
    "    \"\"\"计算二维互相关运算\"\"\"\n",
    "    h, w = K.shape # 获取卷积核的高度和宽度\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)) # 初始化输出矩阵 Y\n",
    "    for i in range(Y.shape[0]): # 遍历输出矩阵的每一行\n",
    "        for j in range(Y.shape[1]): # 遍历输出矩阵的每一列\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum() # 执行互相关运算并赋值给Y的相应位置\n",
    "    return Y\n",
    "\n",
    "# 定义一个二维卷积层类\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__() # 调用nn.Module的构造函数\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size)) # 初始化卷积参数\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x): # 定义前向传播函数\n",
    "        return corr2d(x, self.weight) + self.bias # 返回卷积运算结果\n",
    "\n",
    "# 创建一个6x8的矩阵，中间4列为0\n",
    "X = d2l.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "\n",
    "# 创建一个1x2的卷积核\n",
    "K = d2l.tensor([[1.0, -1.0]])\n",
    "\n",
    "# 进行卷积运算\n",
    "Y = corr2d(X, K)\n",
    "\n",
    "# 对X进行转置，然后再次进行卷积运算\n",
    "corr2d(d2l.transpose(X), K)\n",
    "\n",
    "# 创建一个卷积层，具有1个输出通道和（1, 2）形状的卷积核\n",
    "conv2d = Conv2D(kernel_size=(1, 2))\n",
    "\n",
    "\n",
    "# 调整X和Y的形状以符合PyTorch卷积层的输入输出要求\n",
    "X = X.reshape((1, 1, 6, 8))\n",
    "Y = Y.reshape((1, 1, 6, 7))\n",
    "\n",
    "# 设置学习率\n",
    "lr = 3e-2\n",
    "\n",
    "# 尝试进行10次迭代\n",
    "try:\n",
    "    for i in range(10):\n",
    "        Y_hat = conv2d(X) # 前向传播\n",
    "        l = (Y_hat - Y) ** 2 # 计算损失\n",
    "        conv2d.zero_grad() # 梯度消失\n",
    "        l.sum().backward() # 反向传播\n",
    "        conv2d.weight.data[:] -= lr * conv2d.weight.grad # 更新权重\n",
    "        if (i + 1) % 2 == 0: # 每两个epoch打印一次损失值\n",
    "            print(f'epoch {i+1}, loss {l.sum():.3f}')\n",
    "except Exception as e: # 捕获并打印任何异常\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c4fdc-6a04-4422-9663-07d21e466c4e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;可以通过下述方式调整。参数 X 和 Y 去掉批次大小和通道。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68a01073-3181-4a3b-b69a-0b43ded80557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 18.856\n",
      "epoch 4, loss 6.629\n",
      "epoch 6, loss 2.531\n",
      "epoch 8, loss 1.006\n",
      "epoch 10, loss 0.407\n"
     ]
    }
   ],
   "source": [
    "# 代码验证\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "# 定义二维互相关运算函数\n",
    "def corr2d(X, K):\n",
    "    \"\"\"计算二维互相关运算\"\"\"\n",
    "    h, w = K.shape # 获取卷积核的高度和宽度\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)) # 初始化输出矩阵 Y\n",
    "    for i in range(Y.shape[0]): # 遍历输出矩阵的每一行\n",
    "        for j in range(Y.shape[1]): # 遍历输出矩阵的每一列\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum() # 执行互相关运算并赋值给Y的相应位置\n",
    "    return Y\n",
    "\n",
    "# 定义一个二维卷积层类\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__() # 调用超类的构造函数\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size)) # 随机初始化卷积的权重\n",
    "        self.bias = nn.Parameter(torch.zeros(1)) # 初始化偏置为0\n",
    "\n",
    "    def forward(self, x): # 定义前向传播函数\n",
    "        return corr2d(x, self.weight) + self.bias # 返回卷积运算结果\n",
    "\n",
    "# 创建一个6x8的矩阵，中间4列为0\n",
    "X = d2l.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "\n",
    "# 创建一个1x2的卷积核\n",
    "K = d2l.tensor([[1.0, -1.0]])\n",
    "\n",
    "# 进行卷积运算\n",
    "Y = corr2d(X, K)\n",
    "\n",
    "# 对X进行转置，然后再次进行卷积运算\n",
    "corr2d(d2l.transpose(X), K)\n",
    "\n",
    "# 创建一个卷积层，具有1个输出通道和（1, 2）形状的卷积核\n",
    "conv2d = Conv2D(kernel_size=(1, 2))\n",
    "\n",
    "\n",
    "# 调整X和Y的形状以符合PyTorch卷积层的输入输出要求\n",
    "X = X.reshape((6, 8))\n",
    "Y = Y.reshape((6, 7))\n",
    "\n",
    "# 设置学习率\n",
    "lr = 3e-2\n",
    "\n",
    "# 尝试进行10次迭代\n",
    "try:\n",
    "    for i in range(10):\n",
    "        Y_hat = conv2d(X) # 前向传播\n",
    "        l = (Y_hat - Y) ** 2 # 计算损失\n",
    "        conv2d.zero_grad() # 梯度消失\n",
    "        l.sum().backward() # 反向传播\n",
    "        conv2d.weight.data[:] -= lr * conv2d.weight.grad # 更新权重\n",
    "        if (i + 1) % 2 == 0: # 每两个epoch打印一次损失值\n",
    "            print(f'epoch {i+1}, loss {l.sum():.3f}')\n",
    "except Exception as e: # 捕获并打印任何异常\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cfa18b-4c1f-4704-892c-ffdb17d5a2ad",
   "metadata": {},
   "source": [
    "### 练习三\n",
    "\n",
    "3. 如何通过改变输入张量和卷积核张量，将互相关运算表示为矩阵乘法？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4985165-3c4f-4389-aa78-5a4e32e56b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始：tensor([[1., 0., 1.],\n",
      "        [1., 0., 1.]])\n",
      "outh:1\n",
      "outw:1\n",
      "tensor([1., 0., 1., 1., 0., 1.])\n",
      "Y:torch.stack:tensor([[1., 0., 1., 1., 0., 1.]])\n",
      "K:tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "tensor([[4.]])\n"
     ]
    }
   ],
   "source": [
    "# 代码验证\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "# 定义一个函数来通过矩阵乘法实现二维卷积操作\n",
    "def conv2d_by_mul(X, K):\n",
    "    # 获取卷积核大小\n",
    "    h, w = K.shape\n",
    "    # 计算输出图像大小\n",
    "    outh = X.shape[0] - h + 1\n",
    "    outw = X.shape[1] - w + 1\n",
    "    print(f'outh:{outh}')\n",
    "    print(f'outw:{outw}')\n",
    "    # 调整卷积核形状以便做乘法\n",
    "    K = K.reshape(-1, 1)\n",
    "    # 将输入图像切成卷积核大小的块打平成一维，存放在列表 Y 中\n",
    "    Y = []\n",
    "    for i in range(outh):\n",
    "        for j in range(outw):\n",
    "            Y.append(X[i:i + h, j:j + w].reshape(-1))\n",
    "            print(X[i:i + h, j:j + w].reshape(-1))\n",
    "        # 将列表 Y 转为张量，每行代表一块的打平结果\n",
    "        Y = torch.stack(Y, 0)\n",
    "        print(f'Y:torch.stack:{Y}')\n",
    "        # 用矩阵乘法表示互相关运算\n",
    "        print(f'K:{K}')\n",
    "        res = (torch.matmul(Y, K)).reshape(outh, outw)\n",
    "        # 返回输出结果\n",
    "        return res\n",
    "\n",
    "# 示例\n",
    "# 创建一个2x3的输入图像 X其中第二列的元素被设置为0\n",
    "X = torch.ones((2, 3))\n",
    "X[:, 1] = 0 # 将第二列的所有元素设置为0\n",
    "# 打印输入图像 X\n",
    "print(f'原始：{X}')\n",
    "# 创建一个2x3的卷积核 K，所有元素均为1\n",
    "K = torch.ones((2, 3))\n",
    "# 调用自定义的 conv2d_by_mul 函数来执行二维互相关运算\n",
    "output = conv2d_by_mul(X, K)\n",
    "# 打印互相关运算的结果\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc07d540-d46a-497d-b08b-45e7272a7990",
   "metadata": {},
   "source": [
    "### 练习四\n",
    "\n",
    "4. 手工设计一些卷积核。\n",
    "    1. 二阶导数的核的形式是什么？\n",
    "    1. 积分的核的形式是什么？\n",
    "    1. 得到$d$次导数的最小核的大小是多少？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f150c6-c2ed-4d29-932a-5f1e9503dd6d",
   "metadata": {},
   "source": [
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fce020-db9b-4b59-8a17-ebd365abd664",
   "metadata": {},
   "source": [
    "**第1问：**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c545a7-d69d-4459-9a2b-c34b2e7e5238",
   "metadata": {},
   "source": [
    "&emsp;&emsp;一维的二阶导数的核的形式是：\n",
    "\n",
    "$$\\begin{bmatrix}-1, & 2, & -1,\\end{bmatrix}$$\n",
    "\n",
    "&emsp;&emsp;这个一维卷积可以用于计算信号或函数在每个点的二阶导数。它通过对信号进行两次微分来强调信号的曲率和变化率，通常用于边缘检测和特征检测等任务。\n",
    "\n",
    "&emsp;&emsp;在二维情况下，可以使用以下卷积核来计算图像的二阶导数：\n",
    "\n",
    "\\begin{bmatrix}\n",
    "    0, &  1, & 0 \\\\\n",
    "    1, & -4, & 1 \\\\\n",
    "    0, &  1, & 0 \\\\\n",
    "\\end{bmatrix}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ed57c4-bbb5-4a98-acd7-91148eaf1acc",
   "metadata": {},
   "source": [
    "**第2问：**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ff71c6-15fa-42ae-aa93-a88902d9163e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;积分的核的形式是：\n",
    "\n",
    "$$\\begin{bmatrix}1 & 1 & 1 & \\cdots & 1\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16135f6d-397b-4e5f-bbcc-dfe4509ee789",
   "metadata": {},
   "source": [
    "**第3问：**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6c684a-6fdc-4a90-bea4-9407bc3d5b60",
   "metadata": {},
   "source": [
    "&emsp;&emsp;得到 `d` 次导数的最小核的大小是 $d+1$。例如，一阶导数的最小核大小为$2$，二阶导数的最小核大小为 $3$，三阶导数的最小核大小为 $4$，以此类推。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
